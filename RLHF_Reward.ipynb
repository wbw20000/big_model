{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNfXRSeE6rinfbXjyNfTx6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wbw20000/big_model/blob/main/RLHF_Reward.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk89tLQ4Q4dz",
        "outputId": "80fd905c-98f2-4eb9-aae6-1ade2b355190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        " #连接到google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#定义路径，并读取.py文件\n",
        "file_path = \"/content/drive/MyDrive/Colab Notebooks/GPT图解学习代码/08_ChatGPT/RLHF_Reward_ChatGPT.py\"\n",
        "with open(file_path, \"r\") as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QCUuJ5QRIo7",
        "outputId": "75d44d43-f487-4b8b-8c02-a5b7df228032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import torch # 导入torch\n",
            "from transformers import GPT2Tokenizer # 导入GPT2分词器\n",
            "from transformers import GPT2LMHeadModel # 导入GPT2语言模型\n",
            "\n",
            "model_name = \"gpt2\"  # 也可以选择其他模型，如\"gpt2-medium\"、\"gpt2-large\"等\n",
            "tokenizer = GPT2Tokenizer.from_pretrained(model_name) # 加载分词器\n",
            "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # 判断是否有可用GPU\n",
            "model = GPT2LMHeadModel.from_pretrained(model_name).to(device) # 将模型加载到设备上（CPU或GPU）\n",
            "vocab = tokenizer.get_vocab() # 获取词汇表\n",
            "\n",
            "# 示例RLHF数据集\n",
            "data = [\n",
            "    {\n",
            "        \"User\": \"What is the capital of France?\",\n",
            "        # \"AI\": \"The capital of France is Paris.\",\n",
            "        \"AI\": \"Paris.\",\n",
            "        \"score\": 5\n",
            "    },\n",
            "    {\n",
            "        \"User\": \"What is the capital of France?\",\n",
            "        \"AI\": \"Rome.\",\n",
            "        \"score\": 1\n",
            "    },\n",
            "    {\n",
            "        \"User\": \"How to cook pasta?\",\n",
            "        # \"AI\": \"To cook pasta, first boil water and then add pasta.\",\n",
            "        \"AI\": \"first boil water.\",\n",
            "        \"score\": 4\n",
            "    },\n",
            "    {\n",
            "        \"User\": \"How to cook pasta?\",\n",
            "        # \"AI\": \"First, turn on the microwave and put the pasta inside.\",\n",
            "        \"AI\": \"microwave.\",\n",
            "        \"score\": 2\n",
            "    }\n",
            "]\n",
            "\n",
            "\n",
            "from torch.utils.data import Dataset  # 导入Pytorch的Dataset\n",
            "class RLHFDataset(Dataset):\n",
            "    def __init__(self, data, tokenizer, vocab):\n",
            "        self.tokenizer = tokenizer  # 分词器\n",
            "        self.vocab = vocab  # 词汇表\n",
            "        self.input_data, self.target_data, self.scores = self.process_data(data)\n",
            "        \n",
            "    def process_data(self, data):        \n",
            "        input_data, target_data, scores = [], [], []       \n",
            "        for conversation in data:\n",
            "            user_question = conversation[\"User\"]\n",
            "            model_answer = conversation[\"AI\"]\n",
            "            score = conversation[\"score\"]\n",
            "\n",
            "            input_tokens = self.tokenizer(f\"{user_question}\", return_tensors=\"pt\")[\"input_ids\"].tolist()[0]\n",
            "            input_tokens = input_tokens + [tokenizer.eos_token_id]\n",
            "            input_data.append(torch.tensor(input_tokens, dtype=torch.long))\n",
            "\n",
            "            target_tokens = self.tokenizer(model_answer, return_tensors=\"pt\")[\"input_ids\"].tolist()[0]\n",
            "            target_tokens = target_tokens + [tokenizer.eos_token_id]\n",
            "            target_data.append(torch.tensor(target_tokens, dtype=torch.long))\n",
            "\n",
            "            scores.append(score)\n",
            "\n",
            "        return input_data, target_data, scores\n",
            "    \n",
            "    def __len__(self):\n",
            "        return len(self.input_data)\n",
            "\n",
            "    def __getitem__(self, idx):\n",
            "        return self.input_data[idx], self.target_data[idx], self.scores[idx]\n",
            "\n",
            "rlhf_dataset = RLHFDataset(data, tokenizer, vocab) # 创建ChatDataset对象，传入文件、分词器和词汇表\n",
            "# 打印数据集中前2个数据示例\n",
            "for i in range(2):\n",
            "    input_example, target_example, _ = rlhf_dataset[i]\n",
            "    print(f\"Example {i + 1}:\")\n",
            "    print(\"Input:\", tokenizer.decode(input_example))\n",
            "    print(\"Target:\", tokenizer.decode(target_example))\n",
            "\n",
            "from torch.utils.data import DataLoader # 导入Dataloader\n",
            "tokenizer.pad_token = '<pad>' # 为分词器添加pad token\n",
            "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('<pad>')\n",
            "# 定义pad_sequence函数，用于将一批序列补齐到相同长度\n",
            "def pad_sequence(sequences, padding_value=0, length=None):\n",
            "    # 计算最大序列长度，如果length参数未提供，则使用输入序列中的最大长度\n",
            "    max_length = max(len(seq) for seq in sequences) if length is None else length    \n",
            "    # 创建一个具有适当形状的全零张量，用于存储补齐后的序列\n",
            "    result = torch.full((len(sequences), max_length), padding_value, dtype=torch.long)    \n",
            "    # 遍历序列，将每个序列的内容复制到结果张量中\n",
            "    for i, seq in enumerate(sequences):\n",
            "        end = len(seq)\n",
            "        result[i, :end] = seq[:end]\n",
            "    return result\n",
            "\n",
            "# 定义collate_fn函数，用于将一个批次的数据整理成适当的形状\n",
            "def collate_fn(batch):\n",
            "    # 从批次中分离源序列、目标序列和分数\n",
            "    sources, targets, scores = zip(*batch)    \n",
            "    # 计算批次中的最大序列长度\n",
            "    max_length = max(max(len(s) for s in sources), max(len(t) for t in targets))    \n",
            "    # 使用 pad_sequence 函数补齐源序列和目标序列\n",
            "    sources = pad_sequence(sources, padding_value=tokenizer.pad_token_id, length=max_length)\n",
            "    targets = pad_sequence(targets, padding_value=tokenizer.pad_token_id, length=max_length)\n",
            "    # 将分数转换为张量\n",
            "    scores = torch.tensor(scores, dtype=torch.float)\n",
            "    # 返回补齐后的源序列、目标序列和分数\n",
            "    return sources, targets, scores\n",
            "\n",
            "# 创建Dataloader\n",
            "batch_size = 2\n",
            "chat_dataloader = DataLoader(rlhf_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
            "\n",
            "# 检查Dataloader输出\n",
            "for input_batch, target_batch, score_batch in chat_dataloader:\n",
            "    print(\"Input batch tensor size:\", input_batch.size())\n",
            "    print(\"Target batch tensor size:\", target_batch.size())\n",
            "    print(\"Score batch tensor size:\", score_batch.size())\n",
            "    break\n",
            "\n",
            "\n",
            "# 奖励函数\n",
            "# def reward_function(predictions, targets, scores):\n",
            "#     correct = (predictions == targets).float() * scores.unsqueeze(1)\n",
            "#     reward = correct.sum(dim=-1) / (targets != tokenizer.pad_token_id).sum(dim=-1).float()\n",
            "#     return reward\n",
            "\n",
            "# def reward_function(predictions, targets, scores):\n",
            "#     correct = (predictions == targets).float()\n",
            "#     num_correct = correct.sum(dim=-1)\n",
            "#     num_total = (targets != tokenizer.pad_token_id).sum(dim=-1).float()\n",
            "#     match_ratio = num_correct / num_total\n",
            "#     reward = match_ratio * scores\n",
            "#     return reward\n",
            "\n",
            "def reward_function(predictions, targets, scores):\n",
            "    correct = (predictions == targets).float() * scores.unsqueeze(1)\n",
            "    reward = correct.sum(dim=-1) / (targets != tokenizer.pad_token_id).sum(dim=-1).float()\n",
            "    return reward / scores.max()\n",
            "\n",
            "\n",
            "import numpy as np\n",
            "import torch.nn as nn\n",
            "import torch.optim as optim\n",
            "# 训练过程\n",
            "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
            "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
            "\n",
            "num_epochs = 100\n",
            "for epoch in range(num_epochs):\n",
            "    epoch_rewards = []\n",
            "    \n",
            "    for batch_idx, (input_batch, target_batch, score_batch) in enumerate(chat_dataloader):\n",
            "        optimizer.zero_grad()\n",
            "        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
            "        score_batch = score_batch.to(device)\n",
            "        \n",
            "        outputs = model(input_batch)\n",
            "        logits = outputs.logits\n",
            "        \n",
            "        _, predicted_tokens = torch.max(logits, dim=-1)\n",
            "        \n",
            "        # 计算奖励\n",
            "        rewards = reward_function(predicted_tokens, target_batch, score_batch)\n",
            "        \n",
            "        # 计算损失\n",
            "        loss = criterion(logits.view(-1, logits.size(-1)), target_batch.view(-1))\n",
            "        \n",
            "        # 计算加权损失\n",
            "        weighted_loss = torch.sum(loss * (1 - rewards)) / rewards.numel()\n",
            "        \n",
            "        # 反向传播和优化\n",
            "        weighted_loss.backward()\n",
            "        # loss.backward()\n",
            "        optimizer.step()\n",
            "        \n",
            "        epoch_rewards.append(rewards.cpu().numpy())\n",
            "    \n",
            "    avg_reward = np.mean(np.concatenate(epoch_rewards))\n",
            "    if (epoch + 1) % 20 == 0:\n",
            "        print(f'Epoch: {epoch + 1:04d}, cost = {weighted_loss:.6f}, avg_reward = {avg_reward:.6f}')\n",
            "\n",
            "\n",
            "\n",
            "def generate_text_beam_search(model, input_str, max_len=50, beam_width=5):\n",
            "    model.eval()  # 将模型设置为评估模式（不计算梯度）\n",
            "    # 对输入字符串进行编码，并将其转换为 PyTorch 张量，然后将其移动到相应的设备上（例如 GPU）\n",
            "    input_tokens = tokenizer.encode(input_str, return_tensors=\"pt\").to(device)    \n",
            "    # 初始化候选序列列表，包含当前输入序列和其对数概率得分（我们从0开始）\n",
            "    candidates = [(input_tokens, 0.0)]    \n",
            "    # 禁用梯度计算，以加速预测过程\n",
            "    with torch.no_grad():\n",
            "        # 迭代生成最大长度的序列\n",
            "        for _ in range(max_len):\n",
            "            new_candidates = []            \n",
            "            # 对于每个候选序列\n",
            "            for candidate, candidate_score in candidates:\n",
            "                # 使用模型进行预测\n",
            "                outputs = model(candidate)\n",
            "                # 获取输出 logits\n",
            "                logits = outputs.logits[:, -1, :]\n",
            "                # 获取对数概率得分的 top-k 值（即 beam_width）及其对应的 token\n",
            "                scores, next_tokens = torch.topk(logits, beam_width, dim=-1)\n",
            "                final_results = []\n",
            "                # 遍历 top-k token 及其对应的得分\n",
            "                for score, next_token in zip(scores.squeeze(), next_tokens.squeeze()):\n",
            "                    # 在当前候选序列中添加新的 token\n",
            "                    new_candidate = torch.cat((candidate, next_token.unsqueeze(0).unsqueeze(0)), dim=-1)\n",
            "                    # 更新候选序列的得分\n",
            "                    new_score = candidate_score - score.item()                    \n",
            "                    # 如果新的 token 是结束符（eos_token），则将该候选序列添加到最终结果中\n",
            "                    if next_token.item() == tokenizer.eos_token_id:\n",
            "                        final_results.append((new_candidate, new_score))\n",
            "                    # 否则，将新的候选序列添加到新候选序列列表中\n",
            "                    else:\n",
            "                        new_candidates.append((new_candidate, new_score))            \n",
            "            # 从新候选序列列表中选择得分最高的 top-k 个序列\n",
            "            candidates = sorted(new_candidates, key=lambda x: x[1])[:beam_width]    \n",
            "    # 选择得分最高的候选序列\n",
            "    best_candidate, _ = sorted(candidates, key=lambda x: x[1])[0]    \n",
            "    # 将输出 token 转换回文本字符串\n",
            "    output_str = tokenizer.decode(best_candidate[0])    \n",
            "    # 移除输入字符串并修复空格问题\n",
            "    input_len = len(tokenizer.encode(input_str))\n",
            "    output_str = tokenizer.decode(best_candidate.squeeze()[input_len:])    \n",
            "    return output_str\n",
            "\n",
            "test_inputs = [\n",
            "    \"What is the capital of France?\",\n",
            "    \"How to cook pasta?\",\n",
            "    \"hi , what is your name?\"\n",
            "]\n",
            "\n",
            "for i, input_str in enumerate(test_inputs, start=1):\n",
            "    generated_text = generate_text_beam_search(model, input_str)\n",
            "    print(f\"Test {i}:\")\n",
            "    print(f\"User: {input_str}\")\n",
            "    print(f\"AI: {generated_text}\")\n",
            "    print()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 先运行%load \"/content/drive/MyDrive/Colab Notebooks/GPT图解学习代码/08_ChatGPT/RLHF_Reward_ChatGPT.py\" 就会自动加载.py的代码进来\n",
        "import torch # 导入torch\n",
        "from transformers import GPT2Tokenizer # 导入GPT2分词器\n",
        "from transformers import GPT2LMHeadModel # 导入GPT2语言模型\n",
        "\n",
        "model_name = \"gpt2\"  # 也可以选择其他模型，如\"gpt2-medium\"、\"gpt2-large\"等\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name) # 加载分词器\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # 判断是否有可用GPU\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device) # 将模型加载到设备上（CPU或GPU）\n",
        "vocab = tokenizer.get_vocab() # 获取词汇表\n",
        "\n",
        "# 示例RLHF数据集\n",
        "data = [\n",
        "    {\n",
        "        \"User\": \"What is the capital of France?\",\n",
        "        # \"AI\": \"The capital of France is Paris.\",\n",
        "        \"AI\": \"Paris.\",\n",
        "        \"score\": 5\n",
        "    },\n",
        "    {\n",
        "        \"User\": \"What is the capital of France?\",\n",
        "        \"AI\": \"Rome.\",\n",
        "        \"score\": 1\n",
        "    },\n",
        "    {\n",
        "        \"User\": \"How to cook pasta?\",\n",
        "        # \"AI\": \"To cook pasta, first boil water and then add pasta.\",\n",
        "        \"AI\": \"first boil water.\",\n",
        "        \"score\": 4\n",
        "    },\n",
        "    {\n",
        "        \"User\": \"How to cook pasta?\",\n",
        "        # \"AI\": \"First, turn on the microwave and put the pasta inside.\",\n",
        "        \"AI\": \"microwave.\",\n",
        "        \"score\": 2\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset  # 导入Pytorch的Dataset\n",
        "class RLHFDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, vocab):\n",
        "        self.tokenizer = tokenizer  # 分词器\n",
        "        self.vocab = vocab  # 词汇表\n",
        "        self.input_data, self.target_data, self.scores = self.process_data(data)\n",
        "\n",
        "    def process_data(self, data):\n",
        "        input_data, target_data, scores = [], [], []\n",
        "        for conversation in data:\n",
        "            user_question = conversation[\"User\"]\n",
        "            model_answer = conversation[\"AI\"]\n",
        "            score = conversation[\"score\"]\n",
        "\n",
        "            input_tokens = self.tokenizer(f\"{user_question}\", return_tensors=\"pt\")[\"input_ids\"].tolist()[0]\n",
        "            input_tokens = input_tokens + [tokenizer.eos_token_id]\n",
        "            input_data.append(torch.tensor(input_tokens, dtype=torch.long))\n",
        "\n",
        "            target_tokens = self.tokenizer(model_answer, return_tensors=\"pt\")[\"input_ids\"].tolist()[0]\n",
        "            target_tokens = target_tokens + [tokenizer.eos_token_id]\n",
        "            target_data.append(torch.tensor(target_tokens, dtype=torch.long))\n",
        "\n",
        "            scores.append(score) # 将每句话的整体打分，分配给每个token,让他们的打分跟整句话的分数一样。\n",
        "\n",
        "        return input_data, target_data, scores\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_data[idx], self.target_data[idx], self.scores[idx]\n",
        "\n",
        "rlhf_dataset = RLHFDataset(data, tokenizer, vocab) # 创建ChatDataset对象（实例），这个对象是个数据集包括整理后的input_data,target_data,以及scores等数据，传入文件、分词器和词汇表。不是列表但是可以像列表一样使用。\n",
        "# 打印数据集中前2个数据示例\n",
        "for i in range(2):\n",
        "    input_example, target_example, _ = rlhf_dataset[i]\n",
        "    print(f\"Example {i + 1}:\")\n",
        "    print(\"Input:\", tokenizer.decode(input_example))\n",
        "    print(\"Target:\", tokenizer.decode(target_example))\n",
        "\n",
        "from torch.utils.data import DataLoader # 导入Dataloader\n",
        "tokenizer.pad_token = '<pad>' # 为分词器添加pad token\n",
        "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('<pad>')\n",
        "# 定义pad_sequence函数，用于将一批序列补齐到相同长度\n",
        "def pad_sequence(sequences, padding_value=0, length=None):\n",
        "    # 计算最大序列长度，如果length参数未提供，则使用输入序列中的最大长度\n",
        "    max_length = max(len(seq) for seq in sequences) if length is None else length\n",
        "    # 创建一个具有适当形状的全零张量，用于存储补齐后的序列\n",
        "    result = torch.full((len(sequences), max_length), padding_value, dtype=torch.long)\n",
        "    # 遍历序列，将每个序列的内容复制到结果张量中\n",
        "    for i, seq in enumerate(sequences):\n",
        "        end = len(seq)\n",
        "        result[i, :end] = seq[:end]\n",
        "    return result\n",
        "\n",
        "# 定义collate_fn函数，用于将一个批次的数据整理成适当的形状\n",
        "def collate_fn(batch):\n",
        "    # 从批次中分离源序列、目标序列和分数\n",
        "    sources, targets, scores = zip(*batch)\n",
        "    # 计算批次中的最大序列长度\n",
        "    max_length = max(max(len(s) for s in sources), max(len(t) for t in targets))\n",
        "    # 使用 pad_sequence 函数补齐源序列和目标序列\n",
        "    sources = pad_sequence(sources, padding_value=tokenizer.pad_token_id, length=max_length)\n",
        "    targets = pad_sequence(targets, padding_value=tokenizer.pad_token_id, length=max_length)\n",
        "    # 将分数转换为张量\n",
        "    scores = torch.tensor(scores, dtype=torch.float)\n",
        "    # 返回补齐后的源序列、目标序列和分数\n",
        "    return sources, targets, scores\n",
        "\n",
        "# 创建Dataloader\n",
        "batch_size = 2\n",
        "chat_dataloader = DataLoader(rlhf_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# 检查Dataloader输出\n",
        "for input_batch, target_batch, score_batch in chat_dataloader:\n",
        "    print(\"Input batch tensor size:\", input_batch.size())\n",
        "    print(\"Target batch tensor size:\", target_batch.size())\n",
        "    print(\"Score batch tensor size:\", score_batch.size())\n",
        "    break\n",
        "\n",
        "\n",
        "# 奖励函数\n",
        "# def reward_function(predictions, targets, scores):\n",
        "#     correct = (predictions == targets).float() * scores.unsqueeze(1)\n",
        "#     reward = correct.sum(dim=-1) / (targets != tokenizer.pad_token_id).sum(dim=-1).float()\n",
        "#     return reward\n",
        "\n",
        "# def reward_function(predictions, targets, scores):\n",
        "#     correct = (predictions == targets).float()\n",
        "#     num_correct = correct.sum(dim=-1)\n",
        "#     num_total = (targets != tokenizer.pad_token_id).sum(dim=-1).float()\n",
        "#     match_ratio = num_correct / num_total\n",
        "#     reward = match_ratio * scores\n",
        "#     return reward\n",
        "\n",
        "def reward_function(predictions, targets, scores): #predictions（预测结果），真实 targets（目标序列），以及 scores（人工偏好打分）\n",
        "    correct = (predictions == targets).float() * scores.unsqueeze(1)\n",
        "    # predictions == targets：逐 token 比较，得到一个布尔张量（True/False）\n",
        "    # 举例说明，逐位置对比每个维度的特征\n",
        "    # predictions = torch.tensor([[1, 2, 3], [4, 5, 6]]) ，这里代表2个批次，及2句话的，并行输入所，并行预测出来的对应的2句话。1，2，3 是整数，代表在vocab词汇表中对应的索引。\n",
        "    # targets     = torch.tensor([[1, 0, 3], [4, 0, 6]])\n",
        "    # tensor([[ True, False,  True],\n",
        "    #     [ True, False,  True]])\n",
        "    # tensor([[1.0, 0.0, 1.0],\n",
        "    #    [1.0, 0.0, 1.0]])\n",
        "    #scores.unsqueeze(1) 的矩阵变换\n",
        "    # scores = torch.tensor([0.8, 0.6, 0.9])  # shape: [3]\n",
        "    #tensor([[0.8],\n",
        "    #    [0.6],） # shape: [2, 1]\n",
        "    #最终相乘相当于2，3矩阵乘以2，1矩阵，得到2，1矩阵\n",
        "    #.float()：转为 0.0 / 1.0\n",
        "    #scores.unsqueeze(1)：从 [batch] 变成 [batch, 1]，广播乘法用\n",
        "    #最终的相乘结果，“在每个 token 位置上，如果预测正确，就得到这个样本的 score；如果预测错，得分是 0。\n",
        "    #这是一种 “带权的正确预测掩码”，它实现：\n",
        "    #功能\t描述 正确 token 才有奖励\t错的地方直接乘 0，没奖励\n",
        "    # 分数参与计算\t更好样本有更高奖励（如人类评价好）\n",
        "    # 后续可求总 reward\t用 .sum(dim=-1) 计算每个样本的总得分\n",
        "    reward = correct.sum(dim=-1) / (targets != tokenizer.pad_token_id).sum(dim=-1).float()\n",
        "    return reward / scores.max()\n",
        "    # reward = 最终相乘相当于2，3矩阵乘以2，1矩阵，得到2，1矩阵，([[0.8],\n",
        "    #                                 [0.6],）\n",
        "    # shape: [2, 1]\n",
        "    # 假设scores = torch.tensor([0.8, 0.6, 0.4])\n",
        "    #scores.max() → 0.8\n",
        "    #reward / scores.max()  → [1.0, 0.75, 0.5]\n",
        "    #这就是最终 归一化的 reward（奖励），通常是用来指导策略训练的。\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# 训练过程\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_rewards = []\n",
        "\n",
        "    for batch_idx, (input_batch, target_batch, score_batch) in enumerate(chat_dataloader): #编码后的用户问题（标签序列），编码后的 AI 回答（标签序列），\t每个 token 的打分（RLHF 得分）\n",
        "        optimizer.zero_grad()\n",
        "        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "        score_batch = score_batch.to(device)\n",
        "\n",
        "        outputs = model(input_batch)\n",
        "        logits = outputs.logits # logits：每个 token 对应 vocab 中每个词的打分\n",
        "\n",
        "        _, predicted_tokens = torch.max(logits, dim=-1) # 取每个位置预测概率最大的 token ，#predicted_tokens 是模型输出的序列结果（用于计算 reward）\n",
        "        # torch.max(logits, dim=-1)返回两个值，values, indices = torch.max(input, dim)，\n",
        "        #values: 每个位置上 最大值（即最大概率/logit 分数）\n",
        "        # indices: 每个位置上 最大值的索引，也就是 预测出来的 token ID\n",
        "        #_,是占位符表示，不关心该位置的数据。\n",
        "\n",
        "        # 计算奖励\n",
        "        rewards = reward_function(predicted_tokens, target_batch, score_batch)\n",
        "\n",
        "        # 计算损失\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), target_batch.view(-1)) # criterion 比较 logits（预测分布）和 target（正确词索引）  得到一个交叉熵损失（即预测错误程度）\n",
        "''' 解释criterion\n",
        "| `criterion` 是干嘛的？ | 是“判卷老师”，看你预测和答案差多远                    |\n",
        "| 它怎么判断？            | 把你预测的分数变成概率，然后看你给正确答案的概率是不是高          |\n",
        "| 概率错得多怎么办？         | 扣你更多分（loss大）                          |\n",
        "| 概率越准怎么样？          | 扣分越少（loss小）                           |\n",
        "| 最后模型干嘛？           | 用这些 loss 倒推自己哪里错了，然后慢慢学得更准（这叫反向传播+优化） |\n",
        "'''\n",
        "'''对criterion举例说明\n",
        "想象你在参加一个选择题考试：\n",
        "\n",
        "每题 4 个选项（A、B、C、D）\n",
        "\n",
        "你不写 A/B/C/D，而是告诉老师你觉得每个选项的可能性是多少\n",
        "\n",
        "比如你说：\n",
        "\n",
        "我觉得 A 有 70% 可能是对的，B 20%，C 5%，D 5%\n",
        "\n",
        "如果正确答案就是 A，那老师就会说：\n",
        "\n",
        "你答得挺好，给你扣一点点分（loss 小）\n",
        "\n",
        "但如果你答成：\n",
        "\n",
        "我觉得 A 只有 10%，B 40%，C 40%，D 10%\n",
        "\n",
        "那老师就会说：\n",
        "\n",
        "你完全错了，扣你很多分（loss 大）\n",
        "'''\n",
        "        # 计算加权损失 用 奖励分数（reward） 来决定每个样本的损失要扣多少分：奖励高 → 少扣分，奖励低 → 多扣分。最后把所有人的扣分加起来平均一下，就是 weighted_loss。\n",
        "        # 1 - rewards rewards\n",
        "        # 1.0\t     0.0\t奖励高 → 不惩罚\n",
        "        # rewards\t1 - rewards\t意思\n",
        "        # 0.2\t    0.8\t    奖励低 → 多惩罚\n",
        "        #rewards.numel() 是 reward 中元素的个数（即样本数量或 token 数量）\n",
        "        #torch.sum(...)把所有样本的加权 loss 累加起来\n",
        "        weighted_loss = torch.sum(loss * (1 - rewards)) / rewards.numel()\n",
        "\n",
        "        # 反向传播和优化\n",
        "        weighted_loss.backward()\n",
        "        # loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_rewards.append(rewards.cpu().numpy())\n",
        "\n",
        "    avg_reward = np.mean(np.concatenate(epoch_rewards))\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f'Epoch: {epoch + 1:04d}, cost = {weighted_loss:.6f}, avg_reward = {avg_reward:.6f}')\n",
        "\n",
        "\n",
        "\n",
        "def generate_text_beam_search(model, input_str, max_len=50, beam_width=5):\n",
        "    model.eval()  # 将模型设置为评估模式（不计算梯度）\n",
        "    # 对输入字符串进行编码，并将其转换为 PyTorch 张量，然后将其移动到相应的设备上（例如 GPU）\n",
        "    input_tokens = tokenizer.encode(input_str, return_tensors=\"pt\").to(device)\n",
        "    # 初始化候选序列列表，包含当前输入序列和其对数概率得分（我们从0开始）\n",
        "    candidates = [(input_tokens, 0.0)]\n",
        "    # 禁用梯度计算，以加速预测过程\n",
        "    with torch.no_grad():\n",
        "        # 迭代生成最大长度的序列\n",
        "        for _ in range(max_len):\n",
        "            new_candidates = []\n",
        "            # 对于每个候选序列\n",
        "            for candidate, candidate_score in candidates:\n",
        "                # 使用模型进行预测\n",
        "                outputs = model(candidate)\n",
        "                # 获取输出 logits\n",
        "                logits = outputs.logits[:, -1, :]\n",
        "                # 获取对数概率得分的 top-k 值（即 beam_width）及其对应的 token\n",
        "                scores, next_tokens = torch.topk(logits, beam_width, dim=-1)\n",
        "                final_results = []\n",
        "                # 遍历 top-k token 及其对应的得分\n",
        "                for score, next_token in zip(scores.squeeze(), next_tokens.squeeze()):\n",
        "                    # 在当前候选序列中添加新的 token\n",
        "                    new_candidate = torch.cat((candidate, next_token.unsqueeze(0).unsqueeze(0)), dim=-1)\n",
        "                    # 更新候选序列的得分\n",
        "                    new_score = candidate_score - score.item()\n",
        "                    # 如果新的 token 是结束符（eos_token），则将该候选序列添加到最终结果中\n",
        "                    if next_token.item() == tokenizer.eos_token_id:\n",
        "                        final_results.append((new_candidate, new_score))\n",
        "                    # 否则，将新的候选序列添加到新候选序列列表中\n",
        "                    else:\n",
        "                        new_candidates.append((new_candidate, new_score))\n",
        "            # 从新候选序列列表中选择得分最高的 top-k 个序列\n",
        "            candidates = sorted(new_candidates, key=lambda x: x[1])[:beam_width]\n",
        "    # 选择得分最高的候选序列\n",
        "    best_candidate, _ = sorted(candidates, key=lambda x: x[1])[0]\n",
        "    # 将输出 token 转换回文本字符串\n",
        "    output_str = tokenizer.decode(best_candidate[0])\n",
        "    # 移除输入字符串并修复空格问题\n",
        "    input_len = len(tokenizer.encode(input_str))\n",
        "    output_str = tokenizer.decode(best_candidate.squeeze()[input_len:])\n",
        "    return output_str\n",
        "\n",
        "test_inputs = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"How to cook pasta?\",\n",
        "    \"hi , what is your name?\"\n",
        "]\n",
        "\n",
        "for i, input_str in enumerate(test_inputs, start=1):\n",
        "    generated_text = generate_text_beam_search(model, input_str)\n",
        "    print(f\"Test {i}:\")\n",
        "    print(f\"User: {input_str}\")\n",
        "    print(f\"AI: {generated_text}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "RJdzgokgp81O",
        "outputId": "dec72161-50ac-4049-d719-2f6ab21206d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 242)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m242\u001b[0m\n\u001b[0;31m    avg_reward = np.mean(np.concatenate(epoch_rewards))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 或者通过这条命令直接运行 !python \"/content/drive/MyDrive/Colab Notebooks/GPT图解学习代码/08_ChatGPT/RLHF_Reward_ChatGPT.py\""
      ],
      "metadata": {
        "id": "mqIqtYE7qiJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-xgNfFwqnUi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}